{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5faf532b597641238898364aa8c5814c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_477a375e52154dccb66e97b949d24bef",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e42e8edddc3b42d481a3db68027b7118",
              "IPY_MODEL_3a06da823dbc483ab6b2f7b78a52b66a"
            ]
          }
        },
        "477a375e52154dccb66e97b949d24bef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e42e8edddc3b42d481a3db68027b7118": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_66d0fbe2c373491cb52381ab9318bea9",
            "_dom_classes": [],
            "description": "Epoch:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_25d13e7856324a96ae6842535d7c8051"
          }
        },
        "3a06da823dbc483ab6b2f7b78a52b66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_117964b2d66b466e8920edb32cb11f05",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/2 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7b1edf4991a546268d7392c860ae7a37"
          }
        },
        "66d0fbe2c373491cb52381ab9318bea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "25d13e7856324a96ae6842535d7c8051": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "117964b2d66b466e8920edb32cb11f05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7b1edf4991a546268d7392c860ae7a37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "70adff03bc1a405dbd71c4a16a4d5743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_08adf67f6a234d4fa81e0b9a818876c8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7b3c19632a3a4ac9850d2168237ae956",
              "IPY_MODEL_5d29f494d9dc452aa46515662e443246"
            ]
          }
        },
        "08adf67f6a234d4fa81e0b9a818876c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7b3c19632a3a4ac9850d2168237ae956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_424f1116d94543dea2bec6ac58d2e3a9",
            "_dom_classes": [],
            "description": "Iteration:   1%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 42365,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 423,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5cc1aad0bc634cd391476d187d6979a3"
          }
        },
        "5d29f494d9dc452aa46515662e443246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b261559eee644508a5a0b249378186a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 423/42365 [00:36&lt;1:03:23, 11.03it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_92afa3e380504760831588a1144cd545"
          }
        },
        "424f1116d94543dea2bec6ac58d2e3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5cc1aad0bc634cd391476d187d6979a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b261559eee644508a5a0b249378186a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "92afa3e380504760831588a1144cd545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WQbQukSjSry"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEzQvlXvHWTj"
      },
      "source": [
        "prereqs:\n",
        "- download https://github.com/matt-raporte/anli and save it to your drive under ColabNotebooks/anli\n",
        "- supress disconects step #7 https://towardsdatascience.com/10-tips-for-a-better-google-colab-experience-33f8fe721b82\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBo50yaVW7Ww",
        "outputId": "f040786f-fa82-43fb-ecbd-06a444c7a5cd"
      },
      "source": [
        "# Mount into drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1YXumGLaY2F",
        "outputId": "cdf466a0-7254-4ad4-c5ac-9c463ff3c586"
      },
      "source": [
        "%%bash\n",
        "pip install torch==1.7\n",
        "pip install transformers==3.0.2\n",
        "pip install flint\n",
        "pip install tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.7\n",
            "  Using cached torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (0.6)\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.7.0\n",
            "Collecting transformers==3.0.2\n",
            "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Collecting flint\n",
            "  Downloading flint-0.2.tar.gz (6.4 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from flint) (57.2.0)\n",
            "Collecting pep8>=1.4.3\n",
            "  Downloading pep8-1.7.1-py2.py3-none-any.whl (41 kB)\n",
            "Collecting pyflakes>=0.6.1\n",
            "  Downloading pyflakes-2.3.1-py2.py3-none-any.whl (68 kB)\n",
            "Building wheels for collected packages: flint\n",
            "  Building wheel for flint (setup.py): started\n",
            "  Building wheel for flint (setup.py): finished with status 'done'\n",
            "  Created wheel for flint: filename=flint-0.2-py3-none-any.whl size=6466 sha256=a39a0664e9c761ca5aeb712ef41ed8eb9a5d7c185de2b5427ffb9daebcd5000b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/c1/1f/33c8728944adbac46dcbfb47bc6587fa0b09dd22566061ae57\n",
            "Successfully built flint\n",
            "Installing collected packages: pyflakes, pep8, flint\n",
            "Successfully installed flint-0.2 pep8-1.7.1 pyflakes-2.3.1\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.7.0 which is incompatible.\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n",
            "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeWbTJwMfuPH",
        "outputId": "c23dd457-b9eb-4f90-e7db-26b2691aaaa0"
      },
      "source": [
        "%%bash\n",
        "cd drive/MyDrive/ColabNotebooks/anli\n",
        "source setup.sh\n",
        "bash script/download_data.sh\n",
        "python src/dataset_tools/build_data.py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PYTHONPATH=/env/python:/content/drive/MyDrive/ColabNotebooks/anli/src:/content/drive/MyDrive/ColabNotebooks/anli/utest\n",
            "The path of project root: /content/drive/MyDrive/ColabNotebooks/anli\n",
            "SNLI checked.\n",
            "MNLI checked.\n",
            "FEVER NLI checked.\n",
            "ANLI checked.\n",
            "Data download completed and checked.\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/snli_1.0/snli_1.0_train.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/snli_1.0/snli_1.0_dev.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/snli_1.0/snli_1.0_test.jsonl\n",
            "SNLI examples without gold label have been filtered.\n",
            "SNLI Train size: 549367\n",
            "SNLI Dev size: 9842\n",
            "SNLI Test size: 9824\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/snli/train.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/snli/dev.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/snli/test.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/multinli_1.0/multinli_1.0_train.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/multinli_1.0/multinli_1.0_dev_mismatched.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/multinli_1.0/multinli_1.0_dev_matched.jsonl\n",
            "MNLI examples without gold label have been filtered.\n",
            "MNLI Train size: 392702\n",
            "MNLI MisMatched Dev size: 9832\n",
            "MNLI Matched dev size: 9815\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/mnli/train.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/mnli/mm_dev.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/mnli/m_dev.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/nli_fever/train_fitems.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/nli_fever/dev_fitems.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/nli_fever/test_fitems.jsonl\n",
            "FEVER-NLI Train size: 208346\n",
            "FEVER-NLI Dev size: 19998\n",
            "FEVER-NLI Test size: 19998\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/fever_nli/train.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/fever_nli/dev.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/fever_nli/test.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R1/train.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R1/dev.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R1/test.jsonl\n",
            "ANLI (R1) Train size: 16946\n",
            "ANLI (R1) Dev size: 1000\n",
            "ANLI (R1) Test size: 1000\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r1/train.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r1/dev.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r1/test.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R2/train.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R2/dev.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R2/test.jsonl\n",
            "ANLI (R2) Train size: 45460\n",
            "ANLI (R2) Dev size: 1000\n",
            "ANLI (R2) Test size: 1000\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r2/train.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r2/dev.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r2/test.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R3/train.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R3/dev.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/anli_v1.0/R3/test.jsonl\n",
            "ANLI (R3) Train size: 100459\n",
            "ANLI (R3) Dev size: 1200\n",
            "ANLI (R3) Test size: 1200\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r3/train.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r3/dev.jsonl\n",
            "Save to Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r3/test.jsonl\n",
            "NLI data built!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]\r6960it [00:00, 69595.59it/s]\r13853it [00:00, 69307.70it/s]\r21078it [00:00, 70164.79it/s]\r28660it [00:00, 71767.90it/s]\r35196it [00:00, 67102.99it/s]\r43014it [00:00, 70080.32it/s]\r50994it [00:00, 72737.40it/s]\r58403it [00:00, 73134.89it/s]\r66258it [00:00, 74676.80it/s]\r73495it [00:01, 72676.96it/s]\r80995it [00:01, 73352.79it/s]\r88227it [00:01, 61132.91it/s]\r96002it [00:01, 65320.17it/s]\r103979it [00:01, 69072.59it/s]\r111937it [00:01, 71920.77it/s]\r119702it [00:01, 73548.33it/s]\r127367it [00:01, 74449.43it/s]\r134921it [00:01, 56002.10it/s]\r142249it [00:02, 60263.67it/s]\r150172it [00:02, 64925.65it/s]\r157924it [00:02, 68251.83it/s]\r165637it [00:02, 70690.89it/s]\r173408it [00:02, 72657.90it/s]\r180909it [00:02, 50053.81it/s]\r188582it [00:02, 55882.25it/s]\r196328it [00:02, 60978.04it/s]\r203856it [00:03, 64662.25it/s]\r211394it [00:03, 67541.02it/s]\r219167it [00:03, 70304.28it/s]\r226567it [00:03, 46156.24it/s]\r233995it [00:03, 52070.18it/s]\r241916it [00:03, 58033.66it/s]\r249670it [00:03, 62768.24it/s]\r257276it [00:03, 66240.02it/s]\r264663it [00:04, 68355.53it/s]\r272534it [00:04, 71162.15it/s]\r280460it [00:04, 43451.15it/s]\r288711it [00:04, 50642.99it/s]\r296743it [00:04, 56955.45it/s]\r304621it [00:04, 62116.52it/s]\r311913it [00:04, 62316.88it/s]\r318901it [00:05, 64309.77it/s]\r326643it [00:05, 67537.95it/s]\r333822it [00:05, 66532.56it/s]\r340776it [00:05, 65246.11it/s]\r347576it [00:05, 65444.12it/s]\r354346it [00:05, 66092.77it/s]\r361063it [00:06, 30917.26it/s]\r366179it [00:06, 32045.21it/s]\r371442it [00:06, 36299.38it/s]\r376262it [00:06, 38049.51it/s]\r382608it [00:06, 43243.40it/s]\r390671it [00:06, 50230.09it/s]\r398554it [00:06, 56361.42it/s]\r406585it [00:06, 61898.91it/s]\r414224it [00:06, 65633.01it/s]\r422321it [00:06, 69582.86it/s]\r430386it [00:07, 72569.56it/s]\r438037it [00:07, 73382.69it/s]\r445914it [00:07, 74919.80it/s]\r453609it [00:07, 34770.04it/s]\r461030it [00:07, 41361.31it/s]\r468772it [00:07, 48078.61it/s]\r477063it [00:08, 55011.35it/s]\r484585it [00:08, 59832.81it/s]\r492480it [00:08, 64517.99it/s]\r500691it [00:08, 68948.16it/s]\r508773it [00:08, 72125.67it/s]\r516663it [00:08, 74030.87it/s]\r524500it [00:08, 75258.42it/s]\r532351it [00:08, 76198.77it/s]\r540190it [00:08, 76500.45it/s]\r548352it [00:08, 77965.85it/s]\r550152it [00:09, 61088.39it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  3.13it/s]\r2134it [00:00,  4.46it/s]\r4442it [00:00,  6.38it/s]\r9238it [00:00,  9.11it/s]\r10000it [00:00, 12658.27it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  3.14it/s]\r1062it [00:00,  4.48it/s]\r2888it [00:00,  6.40it/s]\r6831it [00:01,  9.14it/s]\r10000it [00:01, 7257.42it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  3.63it/s]\r3233it [00:00,  5.18it/s]\r6955it [00:00,  7.40it/s]\r11399it [00:00, 10.58it/s]\r15471it [00:00, 15.11it/s]\r20256it [00:00, 21.58it/s]\r23778it [00:00, 30.82it/s]\r27868it [00:01, 44.01it/s]\r33988it [00:01, 62.85it/s]\r38354it [00:01, 89.65it/s]\r44657it [00:01, 127.99it/s]\r49041it [00:01, 182.46it/s]\r53031it [00:01, 260.15it/s]\r56937it [00:01, 370.36it/s]\r62671it [00:01, 527.62it/s]\r66978it [00:02, 749.54it/s]\r71202it [00:02, 1061.97it/s]\r75323it [00:02, 1500.01it/s]\r79772it [00:02, 2112.34it/s]\r85332it [00:02, 2969.28it/s]\r89935it [00:02, 3995.16it/s]\r93775it [00:02, 5245.04it/s]\r99675it [00:02, 7217.89it/s]\r103735it [00:03, 9548.39it/s]\r109775it [00:03, 12775.01it/s]\r116217it [00:03, 16820.39it/s]\r121830it [00:03, 21293.92it/s]\r127365it [00:03, 26113.87it/s]\r132734it [00:03, 29265.12it/s]\r139583it [00:03, 35335.87it/s]\r145124it [00:03, 29074.80it/s]\r149970it [00:04, 33037.64it/s]\r154571it [00:04, 31502.80it/s]\r159685it [00:04, 35604.10it/s]\r164071it [00:04, 36077.51it/s]\r168258it [00:04, 34687.67it/s]\r172144it [00:04, 27765.35it/s]\r178069it [00:04, 33030.53it/s]\r182149it [00:05, 28543.45it/s]\r185797it [00:05, 30535.95it/s]\r189346it [00:05, 15368.65it/s]\r194715it [00:05, 19555.91it/s]\r200257it [00:05, 24266.91it/s]\r205631it [00:06, 29045.17it/s]\r210122it [00:06, 32043.57it/s]\r214787it [00:06, 35364.86it/s]\r219391it [00:06, 38007.41it/s]\r224645it [00:06, 41436.57it/s]\r230115it [00:06, 44686.21it/s]\r236543it [00:06, 49175.72it/s]\r241941it [00:06, 31870.16it/s]\r248435it [00:07, 37616.53it/s]\r255156it [00:07, 43341.04it/s]\r261674it [00:07, 48182.61it/s]\r268092it [00:07, 52076.06it/s]\r274597it [00:07, 55389.91it/s]\r280737it [00:07, 56058.42it/s]\r287293it [00:07, 58606.17it/s]\r293864it [00:07, 60565.46it/s]\r300335it [00:07, 61750.78it/s]\r306688it [00:08, 34010.33it/s]\r313193it [00:08, 39692.14it/s]\r319769it [00:08, 45048.99it/s]\r326443it [00:08, 49913.03it/s]\r332963it [00:08, 53688.46it/s]\r339605it [00:08, 56962.19it/s]\r346377it [00:08, 59811.84it/s]\r352832it [00:08, 60581.56it/s]\r359520it [00:09, 62340.68it/s]\r366001it [00:09, 62630.20it/s]\r372437it [00:09, 62874.91it/s]\r379112it [00:09, 63987.49it/s]\r385601it [00:09, 31132.17it/s]\r391697it [00:09, 36487.98it/s]\r392702it [00:09, 39724.31it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  2.34it/s]\r2049it [00:00,  3.34it/s]\r2528it [00:00,  4.76it/s]\r3682it [00:01,  6.80it/s]\r9632it [00:01,  9.72it/s]\r10000it [00:01, 8680.49it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  3.32it/s]\r2026it [00:00,  4.74it/s]\r2431it [00:00,  6.77it/s]\r8684it [00:00,  9.68it/s]\r10000it [00:00, 12592.06it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  1.89it/s]\r6291it [00:00,  2.71it/s]\r8178it [00:00,  3.87it/s]\r9652it [00:02,  5.51it/s]\r19504it [00:02,  7.88it/s]\r29933it [00:02, 11.25it/s]\r40299it [00:02, 16.07it/s]\r50398it [00:02, 22.96it/s]\r60967it [00:02, 32.80it/s]\r71508it [00:02, 46.85it/s]\r81868it [00:03, 66.92it/s]\r92368it [00:03, 95.57it/s]\r102506it [00:03, 136.47it/s]\r112918it [00:03, 194.85it/s]\r123014it [00:03, 278.12it/s]\r133872it [00:03, 396.88it/s]\r144154it [00:03, 566.00it/s]\r154940it [00:03, 806.75it/s]\r165754it [00:03, 1148.83it/s]\r176545it [00:03, 1633.73it/s]\r187130it [00:04, 2318.37it/s]\r197722it [00:04, 3281.18it/s]\r208346it [00:04, 48752.12it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  3.06it/s]\r4106it [00:00,  4.38it/s]\r7145it [00:00,  6.25it/s]\r15307it [00:00,  8.93it/s]\r19998it [00:00, 24285.96it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  3.79it/s]\r6227it [00:00,  5.41it/s]\r8599it [00:00,  7.73it/s]\r12210it [00:00, 11.04it/s]\r19998it [00:00, 27896.04it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  5.18it/s]\r4190it [00:00,  7.40it/s]\r6258it [00:00, 10.56it/s]\r7681it [00:00, 15.09it/s]\r16029it [00:00, 21.55it/s]\r16946it [00:00, 26249.00it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  5.04it/s]\r347it [00:00,  7.19it/s]\r1000it [00:00, 2653.31it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  6.10it/s]\r349it [00:00,  8.70it/s]\r1000it [00:00, 2843.94it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  4.93it/s]\r6193it [00:00,  7.04it/s]\r8051it [00:00, 10.06it/s]\r9611it [00:00, 14.36it/s]\r18133it [00:00, 20.51it/s]\r26372it [00:00, 29.30it/s]\r34592it [00:01, 41.85it/s]\r43165it [00:01, 59.77it/s]\r45460it [00:01, 38920.85it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  5.10it/s]\r361it [00:00,  7.28it/s]\r1000it [00:00, 2724.31it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  4.83it/s]\r349it [00:00,  6.89it/s]\r1000it [00:00, 2563.41it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  4.51it/s]\r3839it [00:00,  6.44it/s]\r5882it [00:00,  9.20it/s]\r6942it [00:00, 13.14it/s]\r7930it [00:01, 18.71it/s]\r8674it [00:01, 26.67it/s]\r12575it [00:01, 38.09it/s]\r19157it [00:01, 54.40it/s]\r25726it [00:01, 77.69it/s]\r31762it [00:01, 110.93it/s]\r37235it [00:01, 158.33it/s]\r43457it [00:02, 225.94it/s]\r52616it [00:02, 322.43it/s]\r61748it [00:02, 459.92it/s]\r70398it [00:02, 655.53it/s]\r78926it [00:02, 933.40it/s]\r86864it [00:02, 1326.32it/s]\r95295it [00:02, 1882.05it/s]\r100459it [00:02, 36878.88it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  5.45it/s]\r343it [00:00,  7.78it/s]\r1200it [00:00, 3316.38it/s]\n",
            "\r0it [00:00, ?it/s]\r1it [00:00,  5.06it/s]\r349it [00:00,  7.22it/s]\r1200it [00:00, 3114.44it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsdAUPE0O0Iq"
      },
      "source": [
        "#flint.data_utils.fields -> import wasn't working for some reason\n",
        "\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "#\n",
        "# This source code is licensed under Creative Commons-Non Commercial 4.0 found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class FlintField(object):\n",
        "    @classmethod\n",
        "    def batching(cls, batched_data):\n",
        "        raise NotImplemented()\n",
        "\n",
        "\n",
        "class RawFlintField(FlintField):\n",
        "    @classmethod\n",
        "    def batching(cls, batched_data):\n",
        "        return batched_data\n",
        "\n",
        "\n",
        "class LabelFlintField(FlintField):\n",
        "    def batching(self, batched_data):\n",
        "        return torch.tensor(batched_data)\n",
        "\n",
        "\n",
        "class ArrayIndexFlintField(FlintField):\n",
        "    def __init__(self, pad_idx, eos_idx=None, left_pad=False, move_eos_to_beginning=False) -> None:\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.eos_idx = eos_idx\n",
        "        self.left_pad = left_pad\n",
        "        self.move_eos_to_beginning = move_eos_to_beginning\n",
        "\n",
        "    def collate_tokens(self, values, pad_idx, eos_idx=None, left_pad=False, move_eos_to_beginning=False):\n",
        "        \"\"\"\n",
        "        Convert a list of 1d tensors into a padded 2d tensor.\n",
        "        \"\"\"\n",
        "        if not torch.is_tensor(values[0]):\n",
        "            values = [torch.tensor(v) for v in values]\n",
        "\n",
        "        size = max(v.size(0) for v in values)\n",
        "        res = values[0].new(len(values), size).fill_(pad_idx)\n",
        "\n",
        "        def copy_tensor(src, dst):\n",
        "            assert dst.numel() == src.numel()\n",
        "            if move_eos_to_beginning:\n",
        "                assert src[-1] == eos_idx\n",
        "                dst[0] = eos_idx\n",
        "                dst[1:] = src[:-1]\n",
        "            else:\n",
        "                dst.copy_(src)\n",
        "\n",
        "        for i, v in enumerate(values):\n",
        "            copy_tensor(v, res[i][size - len(v):] if left_pad else res[i][:len(v)])\n",
        "        return res\n",
        "\n",
        "    def batching(self, batched_data):\n",
        "        return self.collate_tokens(batched_data,\n",
        "                                   self.pad_idx,\n",
        "                                   self.eos_idx,\n",
        "                                   self.left_pad,\n",
        "                                   self.move_eos_to_beginning)\n",
        "\n",
        "\n",
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "#\n",
        "# This source code is licensed under Creative Commons-Non Commercial 4.0 found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "import torch\n",
        "from typing import Dict, Type\n",
        "\n",
        "\n",
        "class BaseBatchBuilder(object):\n",
        "    def __init__(self, batching_schema: Dict[str, FlintField]) -> None:\n",
        "        super().__init__()\n",
        "        self.batching_schema: Dict[str, FlintField] = batching_schema\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        field_names = batch[0].keys()\n",
        "        batched_data = dict()\n",
        "\n",
        "        for field_name in field_names:\n",
        "            if field_name not in self.batching_schema:\n",
        "                # default is RawFlintField\n",
        "                batched_data[field_name] = RawFlintField.batching([item[field_name] for item in batch])\n",
        "\n",
        "            else:\n",
        "                batched_data[field_name] = self.batching_schema[field_name].batching([item[field_name] for item in batch])\n",
        "\n",
        "        return batched_data\n",
        "\n",
        "\n",
        "def has_tensor(obj) -> bool:\n",
        "    \"\"\"\n",
        "    Given a possibly complex data structure,\n",
        "    check if it has any torch.Tensors in it.\n",
        "    \"\"\"\n",
        "    if isinstance(obj, torch.Tensor):\n",
        "        return True\n",
        "    elif isinstance(obj, dict):\n",
        "        return any(has_tensor(value) for value in obj.values())\n",
        "    elif isinstance(obj, (list, tuple)):\n",
        "        return any(has_tensor(item) for item in obj)\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def move_to_device(obj, cuda_device: int):\n",
        "    \"\"\"\n",
        "    Given a structure (possibly) containing Tensors on the CPU,\n",
        "    move all the Tensors to the specified GPU (or do nothing, if they should be on the CPU).\n",
        "    \"\"\"\n",
        "\n",
        "    if cuda_device < 0 or not has_tensor(obj):\n",
        "        return obj\n",
        "    elif isinstance(obj, torch.Tensor):\n",
        "        return obj.cuda(cuda_device)\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: move_to_device(value, cuda_device) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [move_to_device(item, cuda_device) for item in obj]\n",
        "    elif isinstance(obj, tuple) and hasattr(obj, \"_fields\"):\n",
        "        # This is the best way to detect a NamedTuple, it turns out.\n",
        "        return obj.__class__(*(move_to_device(item, cuda_device) for item in obj))\n",
        "    elif isinstance(obj, tuple):\n",
        "        return tuple(move_to_device(item, cuda_device) for item in obj)\n",
        "    else:\n",
        "        return obj\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zBmc2KLT_BI"
      },
      "source": [
        "## imports and helpers for training\n",
        "\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler, RandomSampler, SequentialSampler\n",
        "#from tqdm import tqdm\n",
        "from tqdm.auto import tqdm, trange\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(\"drive/MyDrive/ColabNotebooks/anli/src\")\n",
        "\n",
        "import config\n",
        "from utils import common, list_dict_data_tool, save_tool\n",
        "#from flint.data_utils.fields import RawFlintField, LabelFlintField, ArrayIndexFlintField\n",
        "\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    \"bert-base\": {\n",
        "        \"model_name\": \"bert-base-uncased\",\n",
        "        \"tokenizer\": BertTokenizer,\n",
        "        \"sequence_classification\": BertForSequenceClassification,\n",
        "        # \"padding_token_value\": 0,\n",
        "        \"padding_segement_value\": 0,\n",
        "        \"padding_att_value\": 0,\n",
        "        \"do_lower_case\": True,\n",
        "    }\n",
        "}\n",
        "\n",
        "registered_path = {\n",
        "    'snli_train': config.PRO_ROOT / \"data/build/snli/train.jsonl\",\n",
        "    'snli_dev': config.PRO_ROOT / \"data/build/snli/dev.jsonl\",\n",
        "    'snli_test': config.PRO_ROOT / \"data/build/snli/test.jsonl\",\n",
        "\n",
        "    'mnli_train': config.PRO_ROOT / \"data/build/mnli/train.jsonl\",\n",
        "    'mnli_m_dev': config.PRO_ROOT / \"data/build/mnli/m_dev.jsonl\",\n",
        "    'mnli_mm_dev': config.PRO_ROOT / \"data/build/mnli/mm_dev.jsonl\",\n",
        "\n",
        "    'fever_train': config.PRO_ROOT / \"data/build/fever_nli/train.jsonl\",\n",
        "    'fever_dev': config.PRO_ROOT / \"data/build/fever_nli/dev.jsonl\",\n",
        "    'fever_test': config.PRO_ROOT / \"data/build/fever_nli/test.jsonl\",\n",
        "\n",
        "    'anli_r1_train': config.PRO_ROOT / \"data/build/anli/r1/train.jsonl\",\n",
        "    'anli_r1_dev': config.PRO_ROOT / \"data/build/anli/r1/dev.jsonl\",\n",
        "    'anli_r1_test': config.PRO_ROOT / \"data/build/anli/r1/test.jsonl\",\n",
        "\n",
        "    'anli_r2_train': config.PRO_ROOT / \"data/build/anli/r2/train.jsonl\",\n",
        "    'anli_r2_dev': config.PRO_ROOT / \"data/build/anli/r2/dev.jsonl\",\n",
        "    'anli_r2_test': config.PRO_ROOT / \"data/build/anli/r2/test.jsonl\",\n",
        "\n",
        "    'anli_r3_train': config.PRO_ROOT / \"data/build/anli/r3/train.jsonl\",\n",
        "    'anli_r3_dev': config.PRO_ROOT / \"data/build/anli/r3/dev.jsonl\",\n",
        "    'anli_r3_test': config.PRO_ROOT / \"data/build/anli/r3/test.jsonl\",\n",
        "}\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "def sample_data_list(d_list, ratio):\n",
        "    if ratio <= 0:\n",
        "        raise ValueError(\"Invalid training weight ratio. Please change --train_weights.\")\n",
        "    upper_int = int(math.ceil(ratio))\n",
        "    if upper_int == 1:\n",
        "        return d_list # if ratio is 1 then we just return the data list\n",
        "    else:\n",
        "        sampled_d_list = []\n",
        "        for _ in range(upper_int):\n",
        "            sampled_d_list.extend(copy.deepcopy(d_list))\n",
        "        if np.isclose(ratio, upper_int):\n",
        "            return sampled_d_list\n",
        "        else:\n",
        "            sampled_length = int(ratio * len(d_list))\n",
        "            random.shuffle(sampled_d_list)\n",
        "            return sampled_d_list[:sampled_length]\n",
        "\n",
        "class NLITransform(object):\n",
        "    def __init__(self, model_name, tokenizer, max_length=None):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        processed_sample = dict()\n",
        "        processed_sample['uid'] = sample['uid']\n",
        "        processed_sample['gold_label'] = sample['label']\n",
        "        processed_sample['y'] = nli_label2index[sample['label']]\n",
        "\n",
        "        # premise: str = sample['premise']\n",
        "        premise: str = sample['context'] if 'context' in sample else sample['premise']\n",
        "        hypothesis: str = sample['hypothesis']\n",
        "\n",
        "        if premise.strip() == '':\n",
        "            premise = 'empty'\n",
        "\n",
        "        if hypothesis.strip() == '':\n",
        "            hypothesis = 'empty'\n",
        "\n",
        "        tokenized_input_seq_pair = self.tokenizer.encode_plus(premise, hypothesis,\n",
        "                                                              max_length=self.max_length,\n",
        "                                                              return_token_type_ids=True, truncation=True)\n",
        "\n",
        "        processed_sample.update(tokenized_input_seq_pair)\n",
        "\n",
        "        return processed_sample\n",
        "\n",
        "def build_eval_dataset_loader_and_sampler(d_list, data_transformer, batching_schema, batch_size_per_gpu_eval):\n",
        "    d_dataset = NLIDataset(d_list, data_transformer)\n",
        "    d_sampler = SequentialSampler(d_dataset)\n",
        "    d_dataloader = DataLoader(dataset=d_dataset,\n",
        "                              batch_size=batch_size_per_gpu_eval,\n",
        "                              shuffle=False,  #\n",
        "                              num_workers=0,\n",
        "                              pin_memory=True,\n",
        "                              sampler=d_sampler,\n",
        "                              collate_fn=BaseBatchBuilder(batching_schema))  #\n",
        "    return d_dataset, d_sampler, d_dataloader\n",
        "\n",
        "class NLIDataset(Dataset):\n",
        "    def __init__(self, data_list, transform) -> None:\n",
        "        super().__init__()\n",
        "        self.d_list = data_list\n",
        "        self.len = len(self.d_list)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        return self.transform(self.d_list[index])\n",
        "\n",
        "    # you should write schema for each of the input elements\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.len\n",
        "\n",
        "nli_label2index = {\n",
        "    'e': 0,\n",
        "    'n': 1,\n",
        "    'c': 2,\n",
        "    'h': -1,\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tkk4T6TIMIb",
        "outputId": "5da9a5c4-2105-4ca9-95ce-a99ec845b80b"
      },
      "source": [
        "## build train set & set model args\n",
        "\n",
        "# Args:\n",
        "local_rank = 0 # this is the gpu rank, we only have 1\n",
        "model_class_name='bert-base' # changed from roberta-base for faster training time https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8\n",
        "adam_epsilon=1e-08\n",
        "cpu=False\n",
        "debug_mode=False\n",
        "epochs=2\n",
        "eval_data='anli_r1_dev:none,anli_r2_dev:none,anli_r3_dev:none'\n",
        "eval_frequency=2000\n",
        "experiment_name='roberta-base|snli+mnli+fnli+r1*10+r2*20+r3*10|nli'\n",
        "fp16=False\n",
        "fp16_opt_level='O1'\n",
        "global_iteration=0\n",
        "gpus_per_node=1\n",
        "gradient_accumulation_steps=4\n",
        "learning_rate=1e-05\n",
        "max_grad_norm=1.0\n",
        "max_length=156\n",
        "node_rank=0\n",
        "num_nodes=1\n",
        "per_gpu_eval_batch_size=16\n",
        "per_gpu_train_batch_size=4\n",
        "resume_path=None\n",
        "sampler_seed=-1\n",
        "save_prediction=True\n",
        "seed=1\n",
        "single_gpu=True\n",
        "total_step=-1\n",
        "#train_data='anli_r1_train:none,anli_r2_train:none,anli_r3_train:none'\n",
        "train_data='anli_r1_train:none'\n",
        "#train_weights='10,20,10'\n",
        "train_weights='10'\n",
        "warmup_steps=-1\n",
        "weight_decay=0.0\n",
        "world_size=1\n",
        "args_dict = {'local_rank':local_rank,'model_class_name':model_class_name,'adam_epsilon':adam_epsilon,'cpu':cpu,'debug_mode':debug_mode,'epochs':epochs,'eval_data':eval_data,'eval_frequency':eval_frequency,'experiment_name':experiment_name,'fp16':fp16,'fp16_opt_level':fp16_opt_level,'global_iteration':global_iteration,'gpus_per_node':gpus_per_node,'gradient_accumulation_steps':gradient_accumulation_steps,'learning_rate':learning_rate,'max_grad_norm':max_grad_norm,'max_length':max_length,'node_rank':node_rank,'num_nodes':num_nodes,'per_gpu_eval_batch_size':per_gpu_eval_batch_size,'per_gpu_train_batch_size':per_gpu_train_batch_size,'resume_path':resume_path,'sampler_seed':sampler_seed,'save_prediction:':save_prediction,'seed':seed,'single_gpu':single_gpu,'total_step':total_step,'train_data':train_data,'train_weights':train_weights,'warmup_steps':warmup_steps,'weight_decay':weight_decay,'world_size': world_size }\n",
        "\n",
        "global_rank = node_rank * gpus_per_node + local_rank\n",
        "local_rank = local_rank\n",
        "# warmup_steps = 20\n",
        "debug_count = 1000\n",
        "\n",
        "if total_step > 0:\n",
        "  num_epoch = 10000  # if we set total step, num_epoch will be forever.\n",
        "else:\n",
        "  num_epoch = epochs\n",
        "\n",
        "actual_train_batch_size = world_size * per_gpu_train_batch_size * gradient_accumulation_steps\n",
        "actual_train_batch_size = actual_train_batch_size\n",
        "\n",
        "set_seed(seed)\n",
        "num_labels = 3      # we are doing NLI so we set num_labels = 3, for other task we can change this value.\n",
        "\n",
        "max_length = max_length\n",
        "\n",
        "model_class_item = MODEL_CLASSES[model_class_name]\n",
        "model_name = model_class_item['model_name']\n",
        "do_lower_case = model_class_item['do_lower_case'] if 'do_lower_case' in model_class_item else False\n",
        "\n",
        "tokenizer = model_class_item['tokenizer'].from_pretrained(model_name,\n",
        "                                                        cache_dir=str(config.PRO_ROOT / \"trans_cache\"),\n",
        "                                                        do_lower_case=do_lower_case)\n",
        "\n",
        "model = model_class_item['sequence_classification'].from_pretrained(model_name,\n",
        "                                                                  cache_dir=str(config.PRO_ROOT / \"trans_cache\"),\n",
        "                                                                  num_labels=num_labels)\n",
        "\n",
        "padding_token_value = tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]\n",
        "padding_segement_value = model_class_item[\"padding_segement_value\"]\n",
        "padding_att_value = model_class_item[\"padding_att_value\"]\n",
        "left_pad = model_class_item['left_pad'] if 'left_pad' in model_class_item else False\n",
        "\n",
        "batch_size_per_gpu_train = per_gpu_train_batch_size\n",
        "batch_size_per_gpu_eval = per_gpu_eval_batch_size\n",
        "\n",
        "if not cpu and not single_gpu:\n",
        "  dist.init_process_group(\n",
        "      backend='nccl',\n",
        "      init_method='env://',\n",
        "      world_size=world_size,\n",
        "      rank=global_rank\n",
        "  )\n",
        "\n",
        "train_data_str = train_data\n",
        "train_data_weights_str = train_weights\n",
        "eval_data_str = eval_data\n",
        "\n",
        "train_data_name = []\n",
        "train_data_path = []\n",
        "train_data_list = []\n",
        "train_data_weights = []\n",
        "\n",
        "eval_data_name = []\n",
        "eval_data_path = []\n",
        "eval_data_list = []\n",
        "\n",
        "train_data_named_path = train_data_str.split(',')\n",
        "weights_str = train_data_weights_str.split(',') if train_data_weights_str is not None else None\n",
        "\n",
        "eval_data_named_path = eval_data_str.split(',')\n",
        "\n",
        "for named_path in train_data_named_path:\n",
        "  ind = named_path.find(':')\n",
        "  name = named_path[:ind]\n",
        "  path = name[ind + 1:]\n",
        "  if name in registered_path: # breaks here\n",
        "      d_list = common.load_jsonl(registered_path[name])\n",
        "  else:\n",
        "      d_list = common.load_jsonl(path)\n",
        "\n",
        "  train_data_name.append(name)\n",
        "  train_data_path.append(path)\n",
        "\n",
        "  train_data_list.append(d_list)\n",
        "\n",
        "if weights_str is not None:\n",
        "  for weights in weights_str:\n",
        "      train_data_weights.append(float(weights))\n",
        "else:\n",
        "  for i in range(len(train_data_list)):\n",
        "      train_data_weights.append(1)\n",
        "\n",
        "for named_path in eval_data_named_path:\n",
        "  ind = named_path.find(':')\n",
        "  name = named_path[:ind]\n",
        "  path = name[ind + 1:]\n",
        "  if name in registered_path:\n",
        "      d_list = common.load_jsonl(registered_path[name])\n",
        "  else:\n",
        "      d_list = common.load_jsonl(path)\n",
        "  eval_data_name.append(name)\n",
        "  eval_data_path.append(path)\n",
        "\n",
        "  eval_data_list.append(d_list)\n",
        "\n",
        "assert len(train_data_weights) == len(train_data_list)\n",
        "\n",
        "batching_schema = {\n",
        "  'uid': RawFlintField(),\n",
        "  'y': LabelFlintField(),\n",
        "  'input_ids': ArrayIndexFlintField(pad_idx=padding_token_value, left_pad=left_pad),\n",
        "  'token_type_ids': ArrayIndexFlintField(pad_idx=padding_segement_value, left_pad=left_pad),\n",
        "  'attention_mask': ArrayIndexFlintField(pad_idx=padding_att_value, left_pad=left_pad),\n",
        "}\n",
        "\n",
        "data_transformer = NLITransform(model_name, tokenizer, max_length)\n",
        "# data_transformer = NLITransform(model_name, tokenizer, max_length, with_element=True)\n",
        "\n",
        "eval_data_loaders = []\n",
        "for eval_d_list in eval_data_list:\n",
        "  d_dataset, d_sampler, d_dataloader = build_eval_dataset_loader_and_sampler(eval_d_list, data_transformer,\n",
        "                                                                              batching_schema,\n",
        "                                                                              batch_size_per_gpu_eval)\n",
        "  eval_data_loaders.append(d_dataloader)\n",
        "\n",
        "# Estimate the training size:\n",
        "training_list = []\n",
        "for i in range(len(train_data_list)):\n",
        "  print(\"Build Training Data ...\")\n",
        "  train_d_list = train_data_list[i]\n",
        "  train_d_name = train_data_name[i]\n",
        "  train_d_weight = train_data_weights[i]\n",
        "  cur_train_list = sample_data_list(train_d_list, train_d_weight)  # change later  # we can apply different sample strategy here.\n",
        "  print(f\"Data Name:{train_d_name}; Weight: {train_d_weight}; \"\n",
        "        f\"Original Size: {len(train_d_list)}; Sampled Size: {len(cur_train_list)}\")\n",
        "  training_list.extend(cur_train_list)\n",
        "estimated_training_size = len(training_list)\n",
        "print(\"Estimated training size:\", estimated_training_size)\n",
        "# Estimate the training size ends:\n",
        "\n",
        "# t_total = estimated_training_size // gradient_accumulation_steps * num_epoch\n",
        "# t_total = estimated_training_size * num_epoch // actual_train_batch_size\n",
        "if total_step <= 0:\n",
        "  t_total = estimated_training_size * num_epoch // actual_train_batch_size\n",
        "else:\n",
        "  t_total = total_step\n",
        "\n",
        "if warmup_steps <= 0:  # set the warmup steps to 0.1 * total step if the given warmup step is -1.\n",
        "  warmup_steps = int(t_total * 0.1)\n",
        "\n",
        "if not cpu:\n",
        "  torch.cuda.set_device(local_rank)\n",
        "  model.cuda(local_rank)\n",
        "\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "  {\n",
        "      \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "      \"weight_decay\": weight_decay,\n",
        "  },\n",
        "  {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
        ")\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "if resume_path:\n",
        "  print(\"Resume Training\")\n",
        "  global_step = global_iteration\n",
        "  print(\"Resume Global Step: \", global_step)\n",
        "  model.load_state_dict(torch.load(str(Path(resume_path) / \"model.pt\"), map_location=torch.device('cpu')))\n",
        "  optimizer.load_state_dict(torch.load(str(Path(resume_path) / \"optimizer.pt\"), map_location=torch.device('cpu')))\n",
        "  scheduler.load_state_dict(torch.load(str(Path(resume_path) / \"scheduler.pt\"), map_location=torch.device('cpu')))\n",
        "  print(\"State Resumed\")\n",
        "\n",
        "if fp16:\n",
        "  try:\n",
        "      from apex import amp\n",
        "  except ImportError:\n",
        "      raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "  model, optimizer = amp.initialize(model, optimizer, opt_level=fp16_opt_level)\n",
        "\n",
        "if not cpu and not single_gpu:\n",
        "  model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank],\n",
        "                                              output_device=local_rank, find_unused_parameters=True)\n",
        "\n",
        "#args_dict = dict(vars(args))\n",
        "file_path_prefix = '.'\n",
        "if global_rank in [-1, 0]:\n",
        "  print(\"Total Steps:\", t_total)\n",
        "  total_step = t_total\n",
        "  print(\"Warmup Steps:\", warmup_steps)\n",
        "  print(\"Actual Training Batch Size:\", actual_train_batch_size)\n",
        "  print(\"Arguments\", pp.pprint(args_dict))\n",
        "\n",
        "is_finished = False\n",
        "\n",
        "# Let build the logger and log everything before the start of the first training epoch.\n",
        "if global_rank in [-1, 0]:  # only do logging if we use cpu or global_rank=0\n",
        "  resume_prefix = \"\"\n",
        "  # if resume_path:\n",
        "  #     resume_prefix = \"resumed_\"\n",
        "\n",
        "  if not debug_mode:\n",
        "      file_path_prefix, date = save_tool.gen_file_prefix(f\"{experiment_name}\")\n",
        "      # # # Create Log File\n",
        "      # Save the source code.\n",
        "      script_name = \"colab_training\" #os.path.basename(__file__)\n",
        "      with open(os.path.join(file_path_prefix, script_name), 'w') as out_f, open(\"drive/MyDrive/ColabNotebooks/anli/colab_train_log.txt\", 'r') as it:\n",
        "          out_f.write(it.read())\n",
        "          out_f.flush()\n",
        "\n",
        "      # Save option file\n",
        "      common.save_json(args_dict, os.path.join(file_path_prefix, \"json\"))\n",
        "      checkpoints_path = Path(file_path_prefix) / \"checkpoints\"\n",
        "      if not checkpoints_path.exists():\n",
        "          checkpoints_path.mkdir()\n",
        "      prediction_path = Path(file_path_prefix) / \"predictions\"\n",
        "      if not prediction_path.exists():\n",
        "          prediction_path.mkdir()\n",
        "\n",
        "      # if this is a resumed, then we save the resumed path.\n",
        "      if resume_path:\n",
        "          with open(os.path.join(file_path_prefix, \"resume_log.txt\"), 'w') as out_f:\n",
        "              out_f.write(str(resume_path))\n",
        "              out_f.flush()\n",
        "\n",
        "# print(f\"Global Rank:{global_rank} ### \", 'Init!')\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "16946it [00:00, 93885.38it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r1/train.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r1/dev.jsonl\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1000it [00:00, 60200.71it/s]\n",
            "1000it [00:00, 55592.72it/s]\n",
            "1200it [00:00, 59737.99it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r2/dev.jsonl\n",
            "Load Jsonl: /content/drive/MyDrive/ColabNotebooks/anli/data/build/anli/r3/dev.jsonl\n",
            "Build Training Data ...\n",
            "Data Name:anli_r1_train; Weight: 10.0; Original Size: 16946; Sampled Size: 169460\n",
            "Estimated training size: 169460\n",
            "Total Steps: 21182\n",
            "Warmup Steps: 2118\n",
            "Actual Training Batch Size: 16\n",
            "{ 'adam_epsilon': 1e-08,\n",
            "  'cpu': False,\n",
            "  'debug_mode': False,\n",
            "  'epochs': 2,\n",
            "  'eval_data': 'anli_r1_dev:none,anli_r2_dev:none,anli_r3_dev:none',\n",
            "  'eval_frequency': 2000,\n",
            "  'experiment_name': 'roberta-base|snli+mnli+fnli+r1*10+r2*20+r3*10|nli',\n",
            "  'fp16': False,\n",
            "  'fp16_opt_level': 'O1',\n",
            "  'global_iteration': 0,\n",
            "  'gpus_per_node': 1,\n",
            "  'gradient_accumulation_steps': 4,\n",
            "  'learning_rate': 1e-05,\n",
            "  'local_rank': 0,\n",
            "  'max_grad_norm': 1.0,\n",
            "  'max_length': 156,\n",
            "  'model_class_name': 'bert-base',\n",
            "  'node_rank': 0,\n",
            "  'num_nodes': 1,\n",
            "  'per_gpu_eval_batch_size': 16,\n",
            "  'per_gpu_train_batch_size': 4,\n",
            "  'resume_path': None,\n",
            "  'sampler_seed': -1,\n",
            "  'save_prediction:': True,\n",
            "  'seed': 1,\n",
            "  'single_gpu': True,\n",
            "  'total_step': -1,\n",
            "  'train_data': 'anli_r1_train:none',\n",
            "  'train_weights': '10',\n",
            "  'warmup_steps': -1,\n",
            "  'weight_decay': 0.0,\n",
            "  'world_size': 1}\n",
            "Arguments None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "5faf532b597641238898364aa8c5814c",
            "477a375e52154dccb66e97b949d24bef",
            "e42e8edddc3b42d481a3db68027b7118",
            "3a06da823dbc483ab6b2f7b78a52b66a",
            "66d0fbe2c373491cb52381ab9318bea9",
            "25d13e7856324a96ae6842535d7c8051",
            "117964b2d66b466e8920edb32cb11f05",
            "7b1edf4991a546268d7392c860ae7a37",
            "70adff03bc1a405dbd71c4a16a4d5743",
            "08adf67f6a234d4fa81e0b9a818876c8",
            "7b3c19632a3a4ac9850d2168237ae956",
            "5d29f494d9dc452aa46515662e443246",
            "424f1116d94543dea2bec6ac58d2e3a9",
            "5cc1aad0bc634cd391476d187d6979a3",
            "b261559eee644508a5a0b249378186a7",
            "92afa3e380504760831588a1144cd545"
          ]
        },
        "id": "HptyjkfXT37f",
        "outputId": "21f5a7b5-c317-4f36-b24e-e80d6cc980c0"
      },
      "source": [
        "for epoch in tqdm(range(num_epoch), desc=\"Epoch\", disable=global_rank not in [-1, 0]):\n",
        "  # Let's build up training dataset for this epoch\n",
        "  training_list = []\n",
        "  for i in range(len(train_data_list)):\n",
        "      print(\"Build Training Data ...\")\n",
        "      train_d_list = train_data_list[i]\n",
        "      train_d_name = train_data_name[i]\n",
        "      train_d_weight = train_data_weights[i]\n",
        "      cur_train_list = sample_data_list(train_d_list, train_d_weight)  # change later  # we can apply different sample strategy here.\n",
        "      print(f\"Data Name:{train_d_name}; Weight: {train_d_weight}; \"\n",
        "            f\"Original Size: {len(train_d_list)}; Sampled Size: {len(cur_train_list)}\")\n",
        "      training_list.extend(cur_train_list)\n",
        "\n",
        "  random.shuffle(training_list)\n",
        "  train_dataset = NLIDataset(training_list, data_transformer)\n",
        "\n",
        "  train_sampler = SequentialSampler(train_dataset)\n",
        "  if not cpu and not single_gpu:\n",
        "      print(\"Use distributed sampler.\")\n",
        "      train_sampler = DistributedSampler(train_dataset, world_size, global_rank,\n",
        "                                          shuffle=True)\n",
        "\n",
        "  train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                                batch_size=batch_size_per_gpu_train,\n",
        "                                shuffle=False,  #\n",
        "                                num_workers=0,\n",
        "                                pin_memory=True,\n",
        "                                sampler=train_sampler,\n",
        "                                collate_fn=BaseBatchBuilder(batching_schema))  #\n",
        "\n",
        "  if not cpu and not single_gpu:\n",
        "      if sampler_seed == -1:\n",
        "          train_sampler.set_epoch(epoch)  # setup the epoch to ensure random sampling at each epoch\n",
        "      else:\n",
        "          train_sampler.set_epoch(epoch + sampler_seed)\n",
        "\n",
        "  for forward_step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\",\n",
        "                                            disable=global_rank not in [-1, 0]), 0):\n",
        "      model.train()\n",
        "\n",
        "      batch = move_to_device(batch, local_rank)\n",
        "      # print(batch['input_ids'], batch['y'])\n",
        "      if model_class_name in [\"distilbert\", \"bart-large\"]:\n",
        "          outputs = model(batch['input_ids'],\n",
        "                          attention_mask=batch['attention_mask'],\n",
        "                          labels=batch['y'])\n",
        "      else:\n",
        "          outputs = model(batch['input_ids'],\n",
        "                          attention_mask=batch['attention_mask'],\n",
        "                          token_type_ids=batch['token_type_ids'],\n",
        "                          labels=batch['y'])\n",
        "      loss, logits = outputs[:2]\n",
        "      # print(debug_node_info(args), loss, logits, batch['uid'])\n",
        "      # print(debug_node_info(args), loss, batch['uid'])\n",
        "\n",
        "      # Accumulated loss\n",
        "      if gradient_accumulation_steps > 1:\n",
        "          loss = loss / gradient_accumulation_steps\n",
        "\n",
        "      # if this forward step need model updates\n",
        "      # handle fp16\n",
        "      if fp16:\n",
        "          with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "              scaled_loss.backward()\n",
        "      else:\n",
        "          loss.backward()\n",
        "\n",
        "          # Gradient clip: if max_grad_norm < 0\n",
        "      if (forward_step + 1) % gradient_accumulation_steps == 0:\n",
        "          if max_grad_norm > 0:\n",
        "              if fp16:\n",
        "                  torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_grad_norm)\n",
        "              else:\n",
        "                  torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "          optimizer.step()\n",
        "          scheduler.step()  # Update learning rate schedule\n",
        "          model.zero_grad()\n",
        "\n",
        "          global_step += 1\n",
        "\n",
        "          if global_rank in [-1, 0] and eval_frequency > 0 and global_step % eval_frequency == 0:\n",
        "              r_dict = dict()\n",
        "              # Eval loop:\n",
        "              for i in range(len(eval_data_name)):\n",
        "                  cur_eval_data_name = eval_data_name[i]\n",
        "                  cur_eval_data_list = eval_data_list[i]\n",
        "                  cur_eval_dataloader = eval_data_loaders[i]\n",
        "                  # cur_eval_raw_data_list = eval_raw_data_list[i]\n",
        "\n",
        "                  evaluation_dataset(args, cur_eval_dataloader, cur_eval_data_list, model, r_dict,\n",
        "                                      eval_name=cur_eval_data_name)\n",
        "\n",
        "              # saving checkpoints\n",
        "              current_checkpoint_filename = \\\n",
        "                  f'e({epoch})|i({global_step})'\n",
        "\n",
        "              for i in range(len(eval_data_name)):\n",
        "                  cur_eval_data_name = eval_data_name[i]\n",
        "                  current_checkpoint_filename += \\\n",
        "                      f'|{cur_eval_data_name}#({round(r_dict[cur_eval_data_name][\"acc\"], 4)})'\n",
        "\n",
        "              if not debug_mode:\n",
        "                  # save model:\n",
        "                  model_output_dir = checkpoints_path / current_checkpoint_filename\n",
        "                  if not model_output_dir.exists():\n",
        "                      model_output_dir.mkdir()\n",
        "                  model_to_save = (\n",
        "                      model.module if hasattr(model, \"module\") else model\n",
        "                  )  # Take care of distributed/parallel training\n",
        "\n",
        "                  torch.save(model_to_save.state_dict(), str(model_output_dir / \"model.pt\"))\n",
        "                  torch.save(optimizer.state_dict(), str(model_output_dir / \"optimizer.pt\"))\n",
        "                  torch.save(scheduler.state_dict(), str(model_output_dir / \"scheduler.pt\"))\n",
        "\n",
        "              # save prediction:\n",
        "              if not debug_mode and save_prediction:\n",
        "                  cur_results_path = prediction_path / current_checkpoint_filename\n",
        "                  if not cur_results_path.exists():\n",
        "                      cur_results_path.mkdir(parents=True)\n",
        "                  for key, item in r_dict.items():\n",
        "                      common.save_jsonl(item['predictions'], cur_results_path / f\"{key}.jsonl\")\n",
        "\n",
        "                  # avoid saving too many things\n",
        "                  for key, item in r_dict.items():\n",
        "                      del r_dict[key]['predictions']\n",
        "                  common.save_json(r_dict, cur_results_path / \"results_dict.json\", indent=2)\n",
        "\n",
        "          if total_step > 0 and global_step == t_total:\n",
        "              # if we set total step and global step s t_total.\n",
        "              is_finished = True\n",
        "              break\n",
        "\n",
        "  # End of epoch evaluation.\n",
        "  if global_rank in [-1, 0] and total_step <= 0:\n",
        "      r_dict = dict()\n",
        "      # Eval loop:\n",
        "      for i in range(len(eval_data_name)):\n",
        "          cur_eval_data_name = eval_data_name[i]\n",
        "          cur_eval_data_list = eval_data_list[i]\n",
        "          cur_eval_dataloader = eval_data_loaders[i]\n",
        "          # cur_eval_raw_data_list = eval_raw_data_list[i]\n",
        "\n",
        "          evaluation_dataset(args, cur_eval_dataloader, cur_eval_data_list, model, r_dict,\n",
        "                              eval_name=cur_eval_data_name)\n",
        "\n",
        "      # saving checkpoints\n",
        "      current_checkpoint_filename = \\\n",
        "          f'e({epoch})|i({global_step})'\n",
        "\n",
        "      for i in range(len(eval_data_name)):\n",
        "          cur_eval_data_name = eval_data_name[i]\n",
        "          current_checkpoint_filename += \\\n",
        "              f'|{cur_eval_data_name}#({round(r_dict[cur_eval_data_name][\"acc\"], 4)})'\n",
        "\n",
        "      if not debug_mode:\n",
        "          # save model:\n",
        "          model_output_dir = checkpoints_path / current_checkpoint_filename\n",
        "          if not model_output_dir.exists():\n",
        "              model_output_dir.mkdir()\n",
        "          model_to_save = (\n",
        "              model.module if hasattr(model, \"module\") else model\n",
        "          )  # Take care of distributed/parallel training\n",
        "\n",
        "          torch.save(model_to_save.state_dict(), str(model_output_dir / \"model.pt\"))\n",
        "          torch.save(optimizer.state_dict(), str(model_output_dir / \"optimizer.pt\"))\n",
        "          torch.save(scheduler.state_dict(), str(model_output_dir / \"scheduler.pt\"))\n",
        "\n",
        "      # save prediction:\n",
        "      if not debug_mode and save_prediction:\n",
        "          cur_results_path = prediction_path / current_checkpoint_filename\n",
        "          if not cur_results_path.exists():\n",
        "              cur_results_path.mkdir(parents=True)\n",
        "          for key, item in r_dict.items():\n",
        "              common.save_jsonl(item['predictions'], cur_results_path / f\"{key}.jsonl\")\n",
        "\n",
        "          # avoid saving too many things\n",
        "          for key, item in r_dict.items():\n",
        "              del r_dict[key]['predictions']\n",
        "          common.save_json(r_dict, cur_results_path / \"results_dict.json\", indent=2)\n",
        "\n",
        "  if is_finished:\n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5faf532b597641238898364aa8c5814c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=2.0, style=ProgressStyle(description_width='i…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Build Training Data ...\n",
            "Data Name:anli_r1_train; Weight: 10.0; Original Size: 16946; Sampled Size: 169460\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70adff03bc1a405dbd71c4a16a4d5743",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=42365.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}